# -*- coding: utf-8 -*-
"""Jackson Mondragon Pardo ExamenFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DfuV6hV_vBbmOrFUIXiPzhUczY9UDJuO

#Problema 1

###Importamos la data
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from sklearn import datasets
from sklearn.model_selection import train_test_split , KFold
from sklearn.preprocessing import Normalizer
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
# importamos iris dataset
iris = datasets.load_iris()
#---------------------
iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
                      columns= iris['feature_names'] + ['target'])
iris_df.head()

"""###Primer modelo"""

"""
Establezca 2 modelos de clasificación para el data Iirs
"""
#Importar librerías
import matplotlib
#matplotlib.use('GTKAgg')
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, recall_score, precision_score
from sklearn.neighbors import KNeighborsClassifier as knn
from sklearn.model_selection import train_test_split as tts
##Importar los datos
dfiris=datasets.load_iris()

##Definir características
X=dfiris.data
y=dfiris.target


##Analizar la dimensionalidad de las variables
#print(np.shape(X))
#print(np.shape(y))
y=np.reshape(y,(150,1))
#print(np.shape(y))


##Entrenar el modelo
Xtrain, Xtest, ytrain, ytest =tts(X, y)

n_neighborss = 1

# import some data to play with

h = .02

modelknn = knn(n_neighbors= n_neighborss)
modelknn.fit(Xtrain,ytrain)

# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00','#00AAFF'])

# We create an instance of Neighbours Classifier and fit the data.

# calculate min, max and limits
x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))

# predict class using data and kNN classifier
Z = modelknn.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("3-Class classification (k = %i)" % (n_neighborss))
plt.show()

"""###Segundo modelo"""

#Importar librerías
import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, recall_score, precision_score
from sklearn.neighbors import KNeighborsClassifier as knn
from sklearn.model_selection import train_test_split as tts
from sklearn import datasets #Me devolverá valores de X features y "y" target para el dataset que importaré. NO me devuelve un df.

##Meter datos

##Definir características
a=np.array(iris_df['sepal length (cm)'])
b=np.array(iris_df['sepal width (cm)'])
c=np.array(iris_df['petal length (cm)'])
d=np.array(iris_df['petal width (cm)'])

X=np.c_[a,b,c,d] 
             
y = np.array(iris_df['target'])


##Analizar la dimensionalidad vectorial
print(np.shape(X))
print('-'*100)
print(np.shape(y))
y = np.reshape(y,(150,1))
print(np.shape(y))


##Dividir el modelo
  ##Separar el target (labels )de los features
Xtrain,Xtest,ytrain,ytest= tts(X,y,test_size=0.2,random_state=12)


##Entrenamiento de la data (Ajuste)
modelknn = knn(n_neighbors = 3)
modelknn.fit(Xtrain,ytrain) 


##Validación del modelo
#np.shape(xtest) ##Para ver el tamaño

  ## Hacer una predicción de los valores de prueba
ypredi1=modelknn.predict(Xtest) #Para test
ypredtrain=modelknn.predict(Xtrain) #Para train
ypredtrain=np.reshape(ypredtrain,(len(ypredtrain),1))
  ##score_total
score_totaltrain = modelknn.score(Xtrain,ytrain)
recallscore=recall_score(ytrain,ypredtrain)
precisionscoretrain=precision_score(ytrain,ypredtrain)

print(f'El score en entrenamiento es: {score_totaltrain}')
print(f'El recall en entrenamiento es: {recallscore}')
print(f'La presicion en entrenamiento es: {precisionscoretrain}')
print('')
#############################################################################

score_totalvalidation = modelknn.score(Xtest,ytest)
recallscoreval=recall_score(ytest,ypredi1)
precisionscoreval=precision_score(ytest,ypredi1)
print('#'*100)
print('')
print(f'El score en validación es: {score_totalvalidation}')
print(f'El recall en validación es: {recallscoreval}')
print(f'La precision en validación es: {precisionscoreval}')

"""#Problema 2

###Con PCA
"""

"""
Evalúa 2 modelos: Uno con PCA y otro sin PCA para el modelo de clasificación del dataset melbournhouses
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

melhouse=pd.read_csv('https://raw.githubusercontent.com/pedrorotta/PythonIntermedio2022/main/Examen/melbournehouses.csv')
display(melhouse.head())
X = np.array(melhouse.drop(columns=['Suburb','Type','Method','SellerG','Date','Regionname','CouncilArea','Address']))
#Todos los valores de la primera fila
print(X[0,:])

##Matriz de dispersión de los datos
#Se necesita un df para poder lograrlo

X_df=melhouse.drop(columns=['Suburb','Type','Method','SellerG','Date','Regionname','CouncilArea','Address'])
pd.plotting.scatter_matrix(X_df,figsize=(30,30),diagonal='kde')
plt.show()

#Escalamiento de datos

escalador=MinMaxScaler()
escalador.fit(X)
X_escalado=escalador.fit_transform(X)


#Visualizar el x escalado la primera fila
print(X_escalado[0,:])

"""###Sin PCA"""

##Data escalada y homogénea
melhouse["Rooms"] = melhouse["Rooms"].fillna(0)
melhouse["Distance"] = melhouse["Distance"].fillna(0)
melhouse["Postcode"] = melhouse["Postcode"].fillna(0)
melhouse["Longtitude"] = melhouse["Longtitude"].fillna(0)
melhouse["Propertycount"] = melhouse["Propertycount"].fillna(0)
melhouse["Bedroom2"] = melhouse["Bedroom2"].fillna(0)
melhouse["Bathroom"] = melhouse["Bathroom"].fillna(0)
melhouse["Car"] = melhouse["Car"].fillna(0)
melhouse["Landsize"] = melhouse["Landsize"].fillna(0)
melhouse["BuildingArea"] = melhouse["BuildingArea"].fillna(0)
melhouse["YearBuilt"] = melhouse["YearBuilt"].fillna(0)
melhouse["Lattitude"] = melhouse["Lattitude"].fillna(0)

X_featuresdf=melhouse.drop(columns=['Suburb','Type','Method','SellerG','Date','Regionname','CouncilArea','Address'])
X_featuresarray=np.array(X_featuresdf)

X_feat_price=X_featuresdf['Price']
X_feat_gen_array = np.array(X_feat_price)

### Encoding => Variables de texto
from sklearn.preprocessing import LabelEncoder
Encoder = LabelEncoder()
Encoder.fit(X_feat_gen_array)
x_gen_encoder = Encoder.fit_transform(X_feat_gen_array)
print(x_gen_encoder)

## Eliminar el X_gen e ingresamos el xgencodificado

X_singen = X_featuresdf.drop(columns = ['Price'])
X_singen['PrecioEscalado'] = x_gen_encoder
print(X_singen.head())


#Escalar los datos
from sklearn.preprocessing import MinMaxScaler as MMS
scaler=MMS()
Xgenerado=np.array(X_singen)
X_scal=scaler.fit_transform(Xgenerado)
#print(X_scal)

##Aplicar K-means
#Codo de jambu => Visualizar el mejor K
from sklearn.cluster import KMeans
inercia=[]
for i in range(1,11):
  k_means=KMeans(n_clusters=i)
  k_means.fit(X_scal)
  inercia.append(k_means.inertia_)
#Visualizar el codo de Jambu

plt.plot(range(1,11),inercia)
plt.show()

##K=4
k_means2=KMeans(n_clusters=4)
k_means2.fit(X_scal)
labels=k_means2.labels_
print(labels)


#Calculamos la forma
np.shape(labels)

#Para graficar, primero PCA
from sklearn.decomposition import PCA

pca3=PCA(n_components=2)
pca3.fit(X_scal)
Xpca=pca3.fit_transform(X_scal)

print(np.shape(Xpca))
print(np.shape(labels))

#Para graficar 

plt.scatter(Xpca[:,0],Xpca[:,1],c=labels)
plt.show()

"""#Problema 3"""

"""
¿Existe sobreajuste al aplicar un modelo de RF con n = 200 para el modelo de wine.csv? 
"""
##RANDOM FOREST REGRESSOR

#from sklearn import datasets #Me devolverá valores de X features y "y" target para el dataset que importaré. NO me devuelve un df.
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split as tts
from sklearn.ensemble import RandomForestRegressor as RFR
from sklearn.metrics import r2_score,mean_squared_error
from sklearn.preprocessing import LabelEncoder
datas= pd.read_excel('/content/wine.xlsx')
display(datas.head())


country=np.array(datas['country'])	
"""
Encoder=LabelEncoder()
Encoder.fit(country)
country=Encoder.fit_transform(country)
"""
points=np.array(datas['points'])
"""
Encoder.fit(points)
points=Encoder.fit_transform(points)
"""
province=np.array(datas['province'])
"""
Encoder.fit(province)
province=Encoder.fit_transform(province)
"""
variety=np.array(datas['variety'])
"""
Encoder.fit(variety)
variety=Encoder.fit_transform(variety)
"""
#X=np.c_[country,points,variety, province]

datas["points"] = datas["points"].fillna(0)
datas["price"] = datas["price"].fillna(0)

X=np.array(points)
#print(x)
y=np.array(datas['price'])
X=np.reshape(X,(len(X),1))
print(np.shape(X))
print(np.shape(y))

##Dividr la data
Xtrain,Xtest,ytrain,ytest=tts(X,y)

##Ajustar la data
modelRF = RFR(random_state=200)
modelRF.fit(Xtrain,ytrain)

##Verificamos los resultados
ypred= modelRF.predict(Xtest)
r2RF=r2_score(ytest,ypred)
print('R2 de validación')
print(r2RF) #Validación
 ## Como evalúo si existe subajuste o sobreajuste
 ## Comparar los resultados de entrenamiento y los de prueba
#Existe subajuste cuando mi r2 estrá entre [0-0.5]
ypredtrain= modelRF.predict(Xtrain)
r2RT=r2_score(ytrain,ypredtrain)
print('R2 de entrenamiento')
print(r2RT) #Entrenamiento

"""<p> No, no existe un sobreajute, existiría más bien un **subajuste**, ya que los métricas de entrenamiento me salen por debajo 0.5 y por encima de 0.0, es decir pertence a una métrica R2 de subajuste. </p>

#Problema 4
"""

"""
Puedes graficar un modelo de deep leraning para la dataset de breast-cancer
"""
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split as tts
from sklearn.metrics import r2_score


breastdf=pd.read_csv('https://raw.githubusercontent.com/pedrorotta/PythonIntermedio2022/main/clase6/breast-cancer.csv')
#display(breastdf.head())

dfx=breastdf.drop(columns=['id','diagnosis'])

from sklearn.preprocessing import LabelEncoder
y=np.array(breastdf['diagnosis'])
Encoder=LabelEncoder()
y=Encoder.fit_transform(y)
y=np.reshape(y,(len(y),1))
#print(y[:4])
#print(np.shape(y))


##Notas: Se puede realizar
##Scatter plot
##Modelos con PCA

from sklearn.preprocessing import MinMaxScaler as MMS
escalar=MMS()
X=np.array(dfx)
X=escalar.fit_transform(X)
#print(X[0:4,:])

#Dividir la data
from sklearn.model_selection import train_test_split as tts
Xtrain,Xtest,ytrain,ytest=tts(X,y)
ModeloClasificador2 = tf.keras.Sequential([
      tf.keras.Input(shape = (30,)),
      tf.keras.layers.Dense(100,activation = 'relu'),
      tf.keras.layers.Dense(400,activation = 'relu'),
      tf.keras.layers.Dense(200,activation = 'relu'),
      tf.keras.layers.Dense(100,activation = 'relu'),
      tf.keras.layers.Dense(50,activation = 'relu'),
      tf.keras.layers.Dense(30,activation = 'relu'),
      tf.keras.layers.Dense(10,activation = 'relu'),
      tf.keras.layers.Dense(4,activation = 'relu'),
      tf.keras.layers.Dense(2,activation = 'relu'),
      tf.keras.layers.Dense(1,activation = 'sigmoid')
])

ModeloClasificador2.compile(loss = tf.keras.losses.binary_crossentropy,optimizer = tf.keras.optimizers.SGD(),
                           metrics = 'accuracy')


historico = ModeloClasificador2.fit(Xtrain,ytrain,epochs = 700,verbose = False)
print('La gráfica dada es:')
pd.DataFrame(historico.history).plot()

"""#Problema 5"""

"""
Crea una función que aplane la ruta de una imágen
"""
import matplotlib.pyplot as plt
def aplanador():
  image=plt.imread(input('Ingrese la ruta de la imagen, por favor: '))
  #plt.imshow(image)
  #plt.show()
  #print(np.shape(image))
  image_vector=image.flatten()
  print('El vector aplando de tu imagen es:')
  print(np.shape(image_vector))
aplanador()